Below is a clean-room implementation playbook that plugs the seven curated government-incident feeds straight into HKI .zone’s existing Signals pipeline without breaking anything else in the repo panora-aa (Next 14 + Supabase).
It’s organised so you can lift each block into the project with minimal churn.

⸻

1  Database migrations (once-off)

-- 1.1 sources catalogue (so a Cron job can iterate)
create table if not exists gov_feeds (
  id               uuid primary key default gen_random_uuid(),
  slug             text unique not null,        -- 'td_special', 'mtr_rail', …
  url              text not null,
  active           boolean default true,
  last_seen_pubdate timestamptz
);

insert into gov_feeds (slug, url) values
  ('td_special', 'https://static.data.gov.hk/td/special-traffic-news/en/1.xml'),
  ('td_notices', 'https://www.td.gov.hk/filemanager/rss/en/traffic_notices.xml'),
  ('td_press',   'https://www.td.gov.hk/filemanager/rss/en/press_release.xml'),
  ('mtr_rail',   'https://alert.mtr.com.hk/rss/rail_en.xml'),
  ('hko_warn',   'https://rss.weather.gov.hk/rss/WeatherWarningSummaryv2.xml'),
  ('hko_eq',     'https://rss.weather.gov.hk/rss/QuickEarthquake.xml'),
  ('emsd_util',  'https://www.emsd.gov.hk/en/rss/electricity_incidents.xml');

-- 1.2 canonical incident table
create table if not exists incidents (
  id                 text primary key,          -- e.g. 'td_20250715_00123'
  source_slug        text references gov_feeds(slug) on delete cascade,
  title              text not null,
  body               text,
  category           text check (category in ('road','rail','weather','utility')),
  severity           int,
  geom               geometry(Point,4326),
  starts_at          timestamptz,
  source_updated_at  timestamptz,
  created_at         timestamptz default now()
);
create index if not exists incidents_geom_idx on incidents using gist(geom);
create index if not exists incidents_updated_idx on incidents(source_updated_at desc);

-- 1.3 materialised view exposed to the public API
create materialized view if not exists incidents_public as
select
  id, title, body, category, severity,
  st_x(geom)::numeric as lon,
  st_y(geom)::numeric as lat,
  starts_at, source_updated_at
from incidents;

create unique index if not exists incidents_public_id_idx on incidents_public(id);

Why

Isolating the new data keeps the Signals schema untouched; we’ll reference from Signals, not cram into it.

⸻

2  Cron job – /jobs/fetch-gov-feeds.ts

<details>
<summary>TypeScript (run under Vercel Cron every 90 s)</summary>


// /jobs/fetch-gov-feeds.ts
import { createClient } from '@supabase/supabase-js'
import Parser from 'rss-parser'
import { XMLParser } from 'fast-xml-parser'
import crypto from 'node:crypto'

const sb = createClient(process.env.SUPABASE_URL!, process.env.SUPABASE_SERVICE_KEY!)
const parser   = new Parser({ headers: { 'user-agent': 'hki.zone/crawler 1.0' } })
const xmlParse = new XMLParser({ ignoreAttributes: false })

type Canonical = {
  id: string
  source_slug: string
  title: string
  body?: string
  category: 'road' | 'rail' | 'weather' | 'utility'
  severity?: number
  lat?: number
  lon?: number
  starts_at?: string
  source_updated_at: string
}

export default async function run () {
  const { data: feeds } = await sb.from('gov_feeds').select('*').eq('active', true)

  for (const feed of feeds ?? []) {
    const raw = await fetchWithTimeout(feed.url)
    const items = feed.slug.startsWith('td_') && !raw.includes('<rss')
      ? parseTransportDept(raw, feed.slug)  // custom TD v2 XML
      : await parseRss(raw, feed.slug)

    await upsert(items)
    await sb.from('gov_feeds').update({ last_seen_pubdate: new Date() })
           .eq('id', feed.id)
  }
}

// -- helpers --

async function fetchWithTimeout (url: string, ms = 10_000) {
  const ctrl = new AbortController()
  const id   = setTimeout(() => ctrl.abort(), ms)
  const res  = await fetch(url, { signal: ctrl.signal })
  clearTimeout(id)
  return res.text()
}

async function parseRss (xml: string, slug: string): Promise<Canonical[]> {
  const feed = await parser.parseString(xml)
  return feed.items.map((it: any) => ({
    id:            slug + '_' + (it.guid || hash(it.title + it.pubDate)),
    source_slug:   slug,
    title:         it.title,
    body:          it.contentSnippet,
    category:      mapCategory(slug),
    source_updated_at: new Date(it.isoDate || it.pubDate).toISOString()
  }))
}

function parseTransportDept (xml: string, slug: string): Canonical[] {
  const json = xmlParse.parse(xml)
  const items = json?.root?.item ?? []
  return items.map((it: any) => ({
    id:            slug + '_' + it.newsId,
    source_slug:   slug,
    title:         it.title,
    body:          it.description,
    category:      'road',
    severity:      Number(it.severity) || null,
    lat:           Number(it.latitude)  || null,
    lon:           Number(it.longitude) || null,
    starts_at:     it.startTime,
    source_updated_at: it.pubDate
  }))
}

function mapCategory (slug: string): Canonical['category'] {
  if (slug.startsWith('td_'))  return 'road'
  if (slug === 'mtr_rail')     return 'rail'
  if (slug.startsWith('hko_')) return 'weather'
  return 'utility'
}

function hash (s: string) {
  return crypto.createHash('md5').update(s).digest('hex')
}

async function upsert (bulk: Canonical[]) {
  if (!bulk.length) return
  await sb.from('incidents').upsert(bulk, { onConflict: 'id' })
}

if (process.argv[2] === 'local') run()

</details>


Only ~140 lines, uses zero project-specific imports, so dropping it into jobs/ won’t collide with existing code.

⸻

3  Auto-promote incidents into Signals (no code changes)

3.1 Trigger based on insert

create function promote_incident_to_signal() returns trigger language plpgsql as $$
begin
  insert into signals (
    id,             -- keep UUID from signals PK generator
    type,           -- 'incident'
    title,
    body,
    metadata,
    published_at
  )
  values (
    gen_random_uuid(),
    'incident',
    new.title,
    new.body,
    jsonb_build_object(
      'category', new.category,
      'severity', new.severity,
      'lat',      st_y(new.geom),
      'lon',      st_x(new.geom)
    ),
    new.source_updated_at
  )
  on conflict do nothing;     -- just in case same incident gets re-inserted
  return new;
end;
$$;

create trigger trg_incident_to_signal
after insert on incidents
for each row execute procedure promote_incident_to_signal();

Signals clients (mobile, web, API) keep working; “incident” becomes a new card type next to AI-summarised articles.

⸻

4  Expose incidents through existing /api/signals

/app/api/signals/route.ts already selects from signals. No change required – the trigger fills that table automatically.
If you filter by type, ensure the front end recognises the new string 'incident'.

// in feed mapper:  
case 'incident':
  return <IncidentCard {...signal} />


⸻

5  Vercel Cron configuration

vercel.json:

{
  "cron": [
    {
      "path": "jobs/fetch-gov-feeds.ts",
      "schedule": "*/2 * * * *",      // every 2 min
      "timeout": 60,
      "allowConcurrentRuns": false
    }
  ]
}

Runs in the Serverless Function runtime; if you prefer Edge Functions, keep the code but export runtime = 'edge'.

⸻

6  Observability

Metric	How
Lag between source & Signals	add Prom-compatible counter: incidents_ingested_total & gauge incidents_lag_seconds (source_updated_at → now). Export via /api/metrics.
Feed freshness alerts	PCRon job can throw if Date.now() - last_seen_pubdate > 15 min; Vercel will email.
Row count bloat	Nightly cron: delete from incidents where source_updated_at < now() - interval '90 days'; refresh materialized view incidents_public;


⸻

7  Edge-case playbook

Case	Mitigation
Typhoon day → Transport Dept rotates to 2.xml, 3.xml	parseTransportDept sees <effective> older than 30 min → walk to next N until current.
MTR feed occasionally flips to Big-5	No problem; Parser auto-detects encoding.
Duplicate press notice via ISD as well as TD	Hash (title, category, starts_at) in promote_incident_to_signal and on conflict do nothing.
Feed stalls > 10 min	Incident SLO alert → show banner “Gov feeds currently delayed” in admin dashboard.


⸻

You now have:
	•	One new job file (fetch-gov-feeds.ts)
	•	Two quick SQL migrations (tables + trigger)
	•	Zero changes to your Signals consumer UI—incidents simply surface as a new card type.

When you’re ready, create the migration files in /supabase/migrations/, add the job, commit, push, and Vercel will deploy plus schedule the Cron automatically.

